<html><!-- Online page at https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/dataproc_job --><head>
                <title>google_dataproc_job</title>
                <meta charset="utf-8"/>
                <link href="../../../../../../style.css" rel="stylesheet"/>
            </head>
            <body>
                <h1 id="google_dataproc_job">google_dataproc_job</h1>

<p>Manages a job resource within a Dataproc cluster within GCE. For more information see
<a href="https://cloud.google.com/dataproc/">the official dataproc documentation</a>.</p>

<aside class="admonition danger">
    <strong>danger</strong>
    <em>danger</em>
    <p>This resource does not support 'update' and changing any attributes will cause the resource to be recreated.</p>
</aside>

<a class="dashAnchor" name="//apple_ref/cpp/Section/Example%20usage"></a><h2 id="example-usage">Example usage</h2>

<div class="codehilite"><pre><span></span><code><span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_cluster"</span><span class="w"> </span><span class="nv">"mycluster"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="na">name</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s2">"dproc-cluster-unique-name"</span><span class="w"></span>
<span class="w">  </span><span class="na">region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"us-central1"</span><span class="w"></span>
<span class="p">}</span><span class="c1"></span>

<span class="c1"># Submit an example spark job to a dataproc cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"spark"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="na">region</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_cluster.mycluster.region</span><span class="w"></span>
<span class="w">  </span><span class="na">force_delete</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">true</span><span class="w"></span>
<span class="w">  </span><span class="nb">placement</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">cluster_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_cluster.mycluster.name</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nb">spark_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">main_class</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s2">"org.apache.spark.examples.SparkPi"</span><span class="w"></span>
<span class="w">    </span><span class="na">jar_file_uris</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">"file:///usr/lib/spark/examples/jars/spark-examples.jar"</span><span class="p">]</span><span class="w"></span>
<span class="w">    </span><span class="na">args</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">"1000"</span><span class="p">]</span><span class="w"></span>

<span class="w">    </span><span class="nb">properties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">"spark.logConf"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"true"</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="nb">logging_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nb">driver_log_levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">"root"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"INFO"</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="c1"></span>

<span class="c1"># Submit an example pyspark job to a dataproc cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"pyspark"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="na">region</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_cluster.mycluster.region</span><span class="w"></span>
<span class="w">  </span><span class="na">force_delete</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">true</span><span class="w"></span>
<span class="w">  </span><span class="nb">placement</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">cluster_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_cluster.mycluster.name</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nb">pyspark_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">main_python_file_uri</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py"</span><span class="w"></span>
<span class="w">    </span><span class="nb">properties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">"spark.logConf"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"true"</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="c1"></span>

<span class="c1"># Check out current state of the jobs</span>
<span class="kr">output</span><span class="w"> </span><span class="nv">"spark_status"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="na">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_job.spark.status[0].state</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kr">output</span><span class="w"> </span><span class="nv">"pyspark_status"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="na">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">google_dataproc_job.pyspark.status[0].state</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<a class="dashAnchor" name="//apple_ref/cpp/Section/Argument%20Reference"></a><h2 id="argument-reference">Argument Reference</h2>

<ul>
<li><p><code>placement.cluster_name</code> - (Required) The name of the cluster where the job 
will be submitted.</p></li>
<li><p><code>xxx_config</code> - (Required) Exactly one of the specific job types to run on the
cluster should be specified. If you want to submit multiple jobs, this will
currently require the definition of multiple <code>google_dataproc_job</code> resources
as shown in the example above, or by setting the <code>count</code> attribute.
The following job configs are supported:</p>

<ul>
<li><a href="#nested_pyspark_config">pyspark_config</a>  - Submits a PySpark job to the cluster</li>
<li><a href="#nested_spark_config">spark_config</a>    - Submits a Spark job to the cluster</li>
<li><a href="#nested_hadoop_config">hadoop_config</a>   - Submits a Hadoop job to the cluster</li>
<li><a href="#nested_hive_config">hive_config</a>     - Submits a Hive job to the cluster</li>
<li><a href="#nested_hpig_config">hpig_config</a>     - Submits a Pig job to the cluster</li>
<li><a href="#nested_sparksql_config">sparksql_config</a> - Submits a Spark SQL job to the cluster</li>
<li><a href="#nested_presto_config">presto_config</a> - Submits a Presto job to the cluster</li>
</ul></li>
</ul>

<hr/>

<ul>
<li><p><code>project</code> - (Optional) The project in which the <code>cluster</code> can be found and jobs
subsequently run against. If it is not provided, the provider project is used.</p></li>
<li><p><code>region</code> - (Optional) The Cloud Dataproc region. This essentially determines which clusters are available
for this job to be submitted to. If not specified, defaults to <code>global</code>.</p></li>
<li><p><code>force_delete</code> - (Optional) By default, you can only delete inactive jobs within
Dataproc. Setting this to true, and calling destroy, will ensure that the
job is first cancelled before issuing the delete.</p></li>
<li><p><code>labels</code> - (Optional) The list of labels (key/value pairs) to add to the job.
<strong>Note</strong>: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.</p></li>
<li><p><code>terraform_labels</code> -
The combination of labels configured directly on the resource and default labels configured on the provider.</p></li>
<li><p><code>effective_labels</code> -
All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.</p></li>
<li><p><code>scheduling.max_failures_per_hour</code> - (Required) Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.</p></li>
<li><p><code>scheduling.max_failures_total</code> - (Required) Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.</p></li>
</ul>

<p><a name="nested_pyspark_config"></a>The <code>pyspark_config</code> block supports:</p>

<p>Submitting a pyspark job to the cluster. Below is an example configuration:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a pyspark job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"pyspark"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">pyspark_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">main_python_file_uri</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py"</span><span class="w"></span>
<span class="w">    </span><span class="nb">properties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">"spark.logConf"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"true"</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<p>For configurations requiring Hadoop Compatible File System (HCFS) references, the options below
are generally applicable:</p>

<pre><code>  - GCS files with the `gs://` prefix
  - HDFS files on the cluster with the `hdfs://` prefix
  - Local files on the cluster with the `file://` prefix
</code></pre>

<ul>
<li><p><code>main_python_file_uri</code>- (Required) The HCFS URI of the main Python file to use as the driver. Must be a .py file.</p></li>
<li><p><code>args</code> - (Optional) The arguments to pass to the driver.</p></li>
<li><p><code>python_file_uris</code> - (Optional) HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.</p></li>
<li><p><code>file_uris</code> - (Optional) HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.</p></li>
<li><p><code>archive_uris</code> - (Optional) HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.</p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in <code>/etc/spark/conf/spark-defaults.conf</code> and classes in user code.</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<p><a name="nested_spark_config"></a>The <code>spark_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a spark job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"spark"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">spark_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">main_class</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s2">"org.apache.spark.examples.SparkPi"</span><span class="w"></span>
<span class="w">    </span><span class="na">jar_file_uris</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">"file:///usr/lib/spark/examples/jars/spark-examples.jar"</span><span class="p">]</span><span class="w"></span>
<span class="w">    </span><span class="na">args</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">"1000"</span><span class="p">]</span><span class="w"></span>

<span class="w">    </span><span class="nb">properties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="s2">"spark.logConf"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"true"</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="nb">logging_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nb">driver_log_levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="s2">"root"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"INFO"</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>main_class</code>- (Optional) The class containing the main method of the driver. Must be in a
provided jar or jar that is already on the classpath. Conflicts with <code>main_jar_file_uri</code></p></li>
<li><p><code>main_jar_file_uri</code> - (Optional) The HCFS URI of jar file containing
the driver jar. Conflicts with <code>main_class</code></p></li>
<li><p><code>args</code> - (Optional) The arguments to pass to the driver.</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.</p></li>
<li><p><code>file_uris</code> - (Optional) HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.</p></li>
<li><p><code>archive_uris</code> - (Optional) HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.</p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in <code>/etc/spark/conf/spark-defaults.conf</code> and classes in user code.</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<p><a name="nested_hadoop_config"></a>The <code>hadoop_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a hadoop job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"hadoop"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">hadoop_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">main_jar_file_uri</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar"</span><span class="w"></span>
<span class="w">    </span><span class="na">args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">"wordcount"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"file:///usr/lib/spark/NOTICE"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"gs://${google_dataproc_cluster.basic.cluster_config[0].bucket}/hadoopjob_output"</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>main_class</code>- (Optional) The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in <code>jar_file_uris</code>. Conflicts with <code>main_jar_file_uri</code></p></li>
<li><p><code>main_jar_file_uri</code> - (Optional) The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with <code>main_class</code></p></li>
<li><p><code>args</code> - (Optional) The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.</p></li>
<li><p><code>file_uris</code> - (Optional) HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.</p></li>
<li><p><code>archive_uris</code> - (Optional) HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.</p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in <code>/etc/hadoop/conf/*-site</code> and classes in user code..</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<p><a name="nested_hive_config"></a>The <code>hive_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a hive job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"hive"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">hive_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">query_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">"DROP TABLE IF EXISTS dprocjob_test"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"CREATE EXTERNAL TABLE dprocjob_test(bar int) LOCATION 'gs://${google_dataproc_cluster.basic.cluster_config[0].bucket}/hive_dprocjob_test/'"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"SELECT * FROM dprocjob_test WHERE bar &gt; 2"</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>query_list</code>- (Optional) The list of Hive queries or statements to execute as part of the job.
Conflicts with <code>query_file_uri</code></p></li>
<li><p><code>query_file_uri</code> - (Optional) HCFS URI of file containing Hive script to execute as the job.
Conflicts with <code>query_list</code></p></li>
<li><p><code>continue_on_failure</code> - (Optional) Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.</p></li>
<li><p><code>script_variables</code> - (Optional) Mapping of query variable names to values (equivalent to the Hive command: <code>SET name="value";</code>).</p></li>
<li><p><code>properties</code> - (Optional)  A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in <code>/etc/hadoop/conf/*-site.xml</code>, <code>/etc/hive/conf/hive-site.xml</code>, and classes in user code..</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.</p></li>
</ul>

<p><a name="nested_pig_config"></a>The <code>pig_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a pig job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"pig"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">pig_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">query_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">"LNS = LOAD 'file:///usr/lib/pig/LICENSE.txt ' AS (line)"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"WORDS = FOREACH LNS GENERATE FLATTEN(TOKENIZE(line)) AS word"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"GROUPS = GROUP WORDS BY word"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"WORD_COUNTS = FOREACH GROUPS GENERATE group, COUNT(WORDS)"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"DUMP WORD_COUNTS"</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>query_list</code>- (Optional) The list of Hive queries or statements to execute as part of the job.
Conflicts with <code>query_file_uri</code></p></li>
<li><p><code>query_file_uri</code> - (Optional) HCFS URI of file containing Hive script to execute as the job.
Conflicts with <code>query_list</code></p></li>
<li><p><code>continue_on_failure</code> - (Optional) Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.</p></li>
<li><p><code>script_variables</code> - (Optional) Mapping of query variable names to values (equivalent to the Pig command: <code>name=[value]</code>).</p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in <code>/etc/hadoop/conf/*-site.xml</code>, <code>/etc/pig/conf/pig.properties</code>, and classes in user code.</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<p><a name="nested_sparksql_config"></a>The <code>sparksql_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a spark SQL job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"sparksql"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">sparksql_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">query_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">"DROP TABLE IF EXISTS dprocjob_test"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"CREATE TABLE dprocjob_test(bar int)"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"SELECT * FROM dprocjob_test WHERE bar &gt; 2"</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>query_list</code>- (Optional) The list of SQL queries or statements to execute as part of the job.
Conflicts with <code>query_file_uri</code></p></li>
<li><p><code>query_file_uri</code> - (Optional) The HCFS URI of the script that contains SQL queries.
Conflicts with <code>query_list</code></p></li>
<li><p><code>script_variables</code> - (Optional) Mapping of query variable names to values (equivalent to the Spark SQL command: <code>SET name="value";</code>).</p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.</p></li>
<li><p><code>jar_file_uris</code> - (Optional) HCFS URIs of jar files to be added to the Spark CLASSPATH.</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<p><a name="nested_presto_config"></a>The <code>presto_config</code> block supports:</p>

<div class="codehilite"><pre><span></span><code><span class="c1"># Submit a Presto job to the cluster</span>
<span class="kr">resource</span><span class="w"> </span><span class="nc">"google_dataproc_job"</span><span class="w"> </span><span class="nv">"presto"</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="p">...</span><span class="w"></span>
<span class="w">  </span><span class="nb">presto_config</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="na">query_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">"DROP TABLE IF EXISTS dprocjob_test"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"CREATE TABLE dprocjob_test(bar int)"</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">"SELECT * FROM dprocjob_test WHERE bar &gt; 2"</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="p">]</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>

<ul>
<li><p><code>client_tags</code> - (Optional) Presto client tags to attach to this query.</p></li>
<li><p><code>continue_on_failure</code> - (Optional) Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.</p></li>
<li><p><code>query_list</code>- (Optional) The list of SQL queries or statements to execute as part of the job.
Conflicts with <code>query_file_uri</code></p></li>
<li><p><code>query_file_uri</code> - (Optional) The HCFS URI of the script that contains SQL queries.
Conflicts with <code>query_list</code></p></li>
<li><p><code>properties</code> - (Optional) A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.</p></li>
<li><p><code>output_format</code> - (Optional) The format in which query output will be displayed. See the Presto documentation for supported output formats.</p></li>
<li><p><code>logging_config.driver_log_levels</code>- (Required) The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'</p></li>
</ul>

<a class="dashAnchor" name="//apple_ref/cpp/Section/Attributes%20Reference"></a><h2 id="attributes-reference">Attributes Reference</h2>

<p>In addition to the arguments listed above, the following computed attributes are
exported:</p>

<ul>
<li><p><code>reference.0.cluster_uuid</code> - A cluster UUID generated by the Cloud Dataproc service when the job is submitted.</p></li>
<li><p><code>status.0.state</code> - A state message specifying the overall job state.</p></li>
<li><p><code>status.0.details</code> - Optional job state details, such as an error description if the state is ERROR.</p></li>
<li><p><code>status.0.state_start_time</code> - The time when this state was entered.</p></li>
<li><p><code>status.0.substate</code> - Additional state information, which includes status reported by the agent.</p></li>
<li><p><code>driver_output_resource_uri</code> - A URI pointing to the location of the stdout of the job's driver program.</p></li>
<li><p><code>driver_controls_files_uri</code> - If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.</p></li>
</ul>

<a class="dashAnchor" name="//apple_ref/cpp/Section/Import"></a><h2 id="import">Import</h2>

<p>This resource does not support import.</p>

<a class="dashAnchor" name="//apple_ref/cpp/Section/Timeouts"></a><h2 id="timeouts">Timeouts</h2>

<p><code>google_dataproc_cluster</code> provides the following
<a href="https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts">Timeouts</a> configuration options: configuration options:</p>

<ul>
<li><code>create</code> - (Default <code>10 minutes</code>) Used for submitting a job to a dataproc cluster.</li>
<li><code>delete</code> - (Default <code>10 minutes</code>) Used for deleting a job from a dataproc cluster.</li>
</ul>

            
        
    </body></html>